import argparse
import logging
import json
from time import sleep
from requests import get
from bs4 import BeautifulSoup as bs

parser = argparse.ArgumentParser(description='Scrape Tripadvisor attractions from a place, their reviews, and reviewer information.')
parser.add_argument('--pid', type=int, help='a TripAdvisor place ID', default=186338)
parser.add_argument('--type', type=str, metavar="type", help='choose an attraction type:\nsl = Sights & Landmarks\ndefault: all', default=all)

args = parser.parse_args()

BASEURL = "https://www.tripadvisor.com/Attractions-g{}-Activities-{}{}-a_allAttractions.true"
TYPE = "" if args.type == "all" else "-c47-"

def generate_page_URLs(n):
    """
    Generates links for every page of the search results.

    Parameters
    ----------
    n : int
        number of pages to be generated
    
    Returns
    -------

    list
         list with URLs to each page of the search results
    """
    pages =  [""] + ["-oa{}-".format(i*30) for i in range(n)][1:]
    return [BASEURL.format(args.pid, TYPE, i) for i in pages]

def get_number_of_pages():
    """
    Finds the number of search result pages given the first page URL.

    Returns
    -------
    int
        number of search result pages

    """
    entrypage = BASEURL.format(args.pid, "", TYPE)
    soup = bs(get(entrypage).content, 'html.parser')
    return int(soup.find('div', {'class': 'pageNumbers'}).findChildren(recursive=True)[-1].get_text())

def do_scrape(urls):
    """
    Scrapes the list of passed URLs and returns a list of dictionaries with the desired attraction info.

    Parameters
    ----------

    urls : list
            list of search result URLS as generated by generate_page_URLs()

    Returns
    -------
    list
        scraped information (attraction URL and name)

    """
    search_results = list()
    for link in urls:
        sleep(1)
        divs = bs(get(link)\
                    .content, 'html.parser')\
                    .find_all("div", {"class" : "_25PvF8uO _2X44Y8hm"})
        
        for div in divs:
            info = dict()
            info["url"] = div.find("div", {"class" : "_2pZeTjmb"}).find("a").get("href")
            info["name"] = div.find("a", {"class" : "_1QKQOve4"}).get_text()
            search_results.append(info)
    
    return search_results


def main():
    logging.basicConfig(filename='attractions.log', level=logging.INFO, filemode="w", format='%(levelname)s:%(message)s')
    logger = logging.getLogger(__name__)
    logger.info("Start scraping...")
    n = get_number_of_pages()
    urls = generate_page_URLs(n)
    results = do_scrape(urls)
    with open("url_list_{}.json".format(args.pid), "w") as f:
        json.dump(results, f)
    logger.info("Finalizing scrape. Shutting down...")

if __name__ == "__main__":
    main()